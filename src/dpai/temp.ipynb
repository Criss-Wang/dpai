{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/criss_w/Desktop/Research_and_ML/MLOps/DeployableAI/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer  # type: ignore\n",
    "model_name = \"GPT2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode([\"what is this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "data = {\"prompt\": [\"Once upon a time\"]}\n",
    "prompt = data[\"prompt\"]\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using the model\n",
    "outputs = model.generate(\n",
    "    input_ids, pad_token_id=tokenizer.pad_token_id, max_length=10, num_return_sequences=1, no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/GPT2/GPT2-2024-02-10-17-57-41.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "\n",
    "def name_with_timestamp(name):\n",
    "    return '{}-{}'.format(name, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "\n",
    "model_artifact = {\n",
    "    \"model\": model,\n",
    "    \"tokenizer\": tokenizer\n",
    "}\n",
    "\n",
    "model_name_with_timestamp = name_with_timestamp(model_name)\n",
    "if not os.path.exists(os.path.join(\"models/\", model_name)):\n",
    "    os.makedirs(os.path.join(\"models/\", model_name))\n",
    "dump(model_artifact, os.path.join(\"models/\", model_name, model_name_with_timestamp + \".joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"once upon a time, we have a lot of content to create. We have to make sure that we're not just going to be able to do it for a while, but we need to have the right tools to get it done.\\n\\n\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\" \n",
    "def input_fn(data):\n",
    "    if not (isinstance(data, list) and len(data) > 0):\n",
    "        data = [data]\n",
    "    inputs = tokenizer(data, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def predict_fn(inputs, model):\n",
    "    # Generate text using the model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "    )\n",
    "\n",
    "    # Decode and print the generated text\n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "prompts = [\"once upon a time, we have a lot of content to create\"]\n",
    "\n",
    "inputs = input_fn(prompts)\n",
    "outputs = predict_fn(inputs, model)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThe first time I saw the new version']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model:\n",
    "    def __init__(self, model_path, inference_path):\n",
    "        self.model_path = model_path\n",
    "        self.inference_path = inference_path\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self):\n",
    "        self._models = {}\n",
    "\n",
    "    def save_to_local(self, model_name, model_path, inference_path, save_local=False):\n",
    "        if not os.path.exists(os.path.join(\"models/\", model_name)):\n",
    "            os.makedirs(os.path.join(\"models/\", model_name))\n",
    "        new_model = Model(model_path, inference_path)\n",
    "        self._models[model_name] = new_model\n",
    "\n",
    "    def register(self, model_name):\n",
    "        pass\n",
    "\n",
    "model_manager = ModelManager()\n",
    "\n",
    "def register_model(model_name, model_path, inference_path):\n",
    "    model_manager.save_to_local(model_name, model_path, inference_path)\n",
    "    model_manager.register(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"-src\",\n",
    "                    required=True,\n",
    "                    help=\"path to some folder\",\n",
    "                    type=os.path.abspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2\n",
      "GPT3\n",
      "GPT4\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = \"configs/model_repository.yml\"\n",
    "def load_config_from_yaml(config_path):\n",
    "    config_path = \"configs/model_repository.yml\"\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "\n",
    "    return config[\"model_names\"]\n",
    "\n",
    "confs = load_config_from_yaml(config_path)\n",
    "for x in confs:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
